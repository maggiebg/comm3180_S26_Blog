[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog is about my explorations and experiments with the Language Models. I’m a sophomore at UPenn, studying Communications and Consumer Psychology."
  },
  {
    "objectID": "posts/002_testing/002_Recipies.html",
    "href": "posts/002_testing/002_Recipies.html",
    "title": "Maggie’s Favorite Recipes",
    "section": "",
    "text": "How Maggie cooks in the Harnwell kitchen:\n\nCooking can be hard for college students, but Maggie has it covered!"
  },
  {
    "objectID": "posts/003_DOW2_big5questions/DOW2_big5.html",
    "href": "posts/003_DOW2_big5questions/DOW2_big5.html",
    "title": "Dataset of the Week #2: Exploring the Big 5 Personality Test",
    "section": "",
    "text": "The Big 5 Personality Test is a part of the Open-Source Psychometrics Project, designed to measure individuals across five core dimensions: Extraversion, Emotional Stability, Agreeableness, Conscientiousness, and Intellect/Imagination. It provides a snapshot into these personality traits that impact our behaviors, thoughts, and emotions.\nIn this case, I will be examining the correlation between Extraversion and Age. In the context of this test, Extroversion is said to be, “an individual’s tendency to seek out social interaction and stimulation, as well as their level of enthusiasm and assertiveness in social situations. Individuals who score high on this dimension tend to be outgoing, sociable, and talkative. They enjoy being around others and seek out social situations. They are often described as having a high level of energy, enthusiasm, and assertiveness. They may also be more likely to engage in risk-taking behaviors, such as partying, drinking, or other forms of excitement-seeking. In contrast, individuals who score low on extraversion are more introverted and reserved. They may prefer to spend time alone or in small groups, and may feel uncomfortable in large social gatherings. They may also be less assertive and more cautious in their interactions with others.”\nIt is proven that extraversion is correlated to multiple outcomes, ranging from job performance and social support to overall well-being. We will further explore these conclusions through analyzing the data from this test.\n\nimport pandas as pd\nimport re\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import pearsonr\n\n\nbig5_df = pd.read_csv('data/openpsych_data.csv', sep='\\t')\n\n\nbig5_df\n\n\n\n\n\n\n\n\nrace\nage\nengnat\ngender\nhand\nsource\ncountry\nE1\nE2\nE3\n...\nO1\nO2\nO3\nO4\nO5\nO6\nO7\nO8\nO9\nO10\n\n\n\n\n0\n3\n53\n1\n1\n1\n1\nUS\n4\n2\n5\n...\n4\n1\n3\n1\n5\n1\n4\n2\n5\n5\n\n\n1\n13\n46\n1\n2\n1\n1\nUS\n2\n2\n3\n...\n3\n3\n3\n3\n2\n3\n3\n1\n3\n2\n\n\n2\n1\n14\n2\n2\n1\n1\nPK\n5\n1\n1\n...\n4\n5\n5\n1\n5\n1\n5\n5\n5\n5\n\n\n3\n3\n19\n2\n2\n1\n1\nRO\n2\n5\n2\n...\n4\n3\n5\n2\n4\n2\n5\n2\n5\n5\n\n\n4\n11\n25\n2\n2\n1\n2\nUS\n3\n1\n3\n...\n3\n1\n1\n1\n3\n1\n3\n1\n5\n3\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n19714\n11\n15\n1\n2\n1\n2\nSG\n1\n4\n3\n...\n1\n3\n5\n3\n4\n1\n4\n2\n5\n3\n\n\n19715\n3\n37\n1\n2\n1\n2\nUS\n2\n3\n2\n...\n1\n2\n3\n2\n3\n3\n4\n2\n3\n3\n\n\n19716\n5\n16\n2\n1\n1\n2\nUS\n2\n5\n4\n...\n5\n3\n1\n3\n4\n1\n1\n5\n5\n5\n\n\n19717\n12\n16\n1\n1\n1\n5\nNG\n1\n4\n2\n...\n3\n2\n5\n3\n4\n1\n5\n3\n5\n5\n\n\n19718\n3\n35\n1\n1\n1\n1\nUS\n2\n3\n1\n...\n5\n1\n5\n1\n4\n1\n5\n5\n5\n5\n\n\n\n\n19719 rows × 57 columns\n\n\n\nParticipants of this survey were tasked to answer 50 of the questions below, 10 questions associated with each of the above traits.\n\nquestions = '''\nE1  I am the life of the party.\nE2  I don't talk a lot.\nE3  I feel comfortable around people.\nE4  I keep in the background.\nE5  I start conversations.\nE6  I have little to say.\nE7  I talk to a lot of different people at parties.\nE8  I don't like to draw attention to myself.\nE9  I don't mind being the center of attention.\nE10 I am quiet around strangers.\nN1  I get stressed out easily.\nN2  I am relaxed most of the time.\nN3  I worry about things.\nN4  I seldom feel blue.\nN5  I am easily disturbed.\nN6  I get upset easily.\nN7  I change my mood a lot.\nN8  I have frequent mood swings.\nN9  I get irritated easily.\nN10 I often feel blue.\nA1  I feel little concern for others.\nA2  I am interested in people.\nA3  I insult people.\nA4  I sympathize with others' feelings.\nA5  I am not interested in other people's problems.\nA6  I have a soft heart.\nA7  I am not really interested in others.\nA8  I take time out for others.\nA9  I feel others' emotions.\nA10 I make people feel at ease.\nC1  I am always prepared.\nC2  I leave my belongings around.\nC3  I pay attention to details.\nC4  I make a mess of things.\nC5  I get chores done right away.\nC6  I often forget to put things back in their proper place.\nC7  I like order.\nC8  I shirk my duties.\nC9  I follow a schedule.\nC10 I am exacting in my work.\nO1  I have a rich vocabulary.\nO2  I have difficulty understanding abstract ideas.\nO3  I have a vivid imagination.\nO4  I am not interested in abstract ideas.\nO5  I have excellent ideas.\nO6  I do not have a good imagination.\nO7  I am quick to understand things.\nO8  I use difficult words.\nO9  I spend time reflecting on things.\nO10 I am full of ideas.\n'''\n\n\nbig5_questions_df = pd.DataFrame([item.split('\\t') for item in questions.splitlines() if item&gt;''])\n\n\nbig5_questions_df\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\nE1\nI am the life of the party.\n\n\n1\nE2\nI don't talk a lot.\n\n\n2\nE3\nI feel comfortable around people.\n\n\n3\nE4\nI keep in the background.\n\n\n4\nE5\nI start conversations.\n\n\n5\nE6\nI have little to say.\n\n\n6\nE7\nI talk to a lot of different people at parties.\n\n\n7\nE8\nI don't like to draw attention to myself.\n\n\n8\nE9\nI don't mind being the center of attention.\n\n\n9\nE10\nI am quiet around strangers.\n\n\n10\nN1\nI get stressed out easily.\n\n\n11\nN2\nI am relaxed most of the time.\n\n\n12\nN3\nI worry about things.\n\n\n13\nN4\nI seldom feel blue.\n\n\n14\nN5\nI am easily disturbed.\n\n\n15\nN6\nI get upset easily.\n\n\n16\nN7\nI change my mood a lot.\n\n\n17\nN8\nI have frequent mood swings.\n\n\n18\nN9\nI get irritated easily.\n\n\n19\nN10\nI often feel blue.\n\n\n20\nA1\nI feel little concern for others.\n\n\n21\nA2\nI am interested in people.\n\n\n22\nA3\nI insult people.\n\n\n23\nA4\nI sympathize with others' feelings.\n\n\n24\nA5\nI am not interested in other people's problems.\n\n\n25\nA6\nI have a soft heart.\n\n\n26\nA7\nI am not really interested in others.\n\n\n27\nA8\nI take time out for others.\n\n\n28\nA9\nI feel others' emotions.\n\n\n29\nA10\nI make people feel at ease.\n\n\n30\nC1\nI am always prepared.\n\n\n31\nC2\nI leave my belongings around.\n\n\n32\nC3\nI pay attention to details.\n\n\n33\nC4\nI make a mess of things.\n\n\n34\nC5\nI get chores done right away.\n\n\n35\nC6\nI often forget to put things back in their pro...\n\n\n36\nC7\nI like order.\n\n\n37\nC8\nI shirk my duties.\n\n\n38\nC9\nI follow a schedule.\n\n\n39\nC10\nI am exacting in my work.\n\n\n40\nO1\nI have a rich vocabulary.\n\n\n41\nO2\nI have difficulty understanding abstract ideas.\n\n\n42\nO3\nI have a vivid imagination.\n\n\n43\nO4\nI am not interested in abstract ideas.\n\n\n44\nO5\nI have excellent ideas.\n\n\n45\nO6\nI do not have a good imagination.\n\n\n46\nO7\nI am quick to understand things.\n\n\n47\nO8\nI use difficult words.\n\n\n48\nO9\nI spend time reflecting on things.\n\n\n49\nO10\nI am full of ideas.\n\n\n\n\n\n\n\nFirst, I categorized all of the questions into which trait they fulfill:\n\nfactor_map = { 1: 'E', \n               2: 'A',\n               3: 'C',\n               4: 'N',\n               5: 'O' }\n\n\nipip_df = pd.read_html('big5_questions.html', header=0)[0]\nipip_df = ipip_df.rename(columns={'Unnamed: 1': 'text', 'Unnamed: 7': 'factor_and_direction'})[['text','factor_and_direction']]\nipip_df[['factor','direction']]=ipip_df['factor_and_direction'].str.extract(r'([1-5])(.)')\nipip_df['category']=ipip_df['factor'].astype(int).map(factor_map)\n\n\nipip_df = ipip_df.assign(number=np.repeat(np.arange(1,11),5))\nipip_df = ipip_df.assign(qcode=ipip_df['category'].str.cat(ipip_df['number'].astype(str)))  \n\n\nneg_items = ipip_df.query('direction==\"-\"')['qcode']\nneg_items\n\n1      A1\n3      N1\n5      E2\n7      C2\n9      O2\n11     A3\n13     N3\n15     E4\n17     C4\n19     O4\n21     A5\n23     N5\n25     E6\n27     C6\n28     N6\n29     O6\n31     A7\n33     N7\n35     E8\n37     C8\n38     N8\n43     N9\n45    E10\n48    N10\nName: qcode, dtype: str\n\n\n\nbig5_scored_df = big5_df.copy()\n\n\nbig5_scored_df[neg_items] = 6-big5_df[neg_items]\n\nThen, I sorted the data to just record the Extraversion responses:\n\nE_cols = [f'E{n+1}' for n in range(10)]\nE_cols\n\n['E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8', 'E9', 'E10']\n\n\n\ncat_cols\n\n{'O': ['O1', 'O2', 'O3', 'O4', 'O5', 'O6', 'O7', 'O8', 'O9', 'O10'],\n 'C': ['C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10'],\n 'E': ['E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8', 'E9', 'E10'],\n 'A': ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10'],\n 'N': ['N1', 'N2', 'N3', 'N4', 'N5', 'N6', 'N7', 'N8', 'N9', 'N10']}\n\n\n\nfor cat, cols in cat_cols.items():\n    big5_scored_df[cat]=big5_scored_df[cols].sum(axis=1)\n\n\nbig5_scored_df[['O','C','E','A','N']]\n\n\n\n\n\n\n\n\nO\nC\nE\nA\nN\n\n\n\n\n0\n43\n47\n44\n46\n49\n\n\n1\n26\n42\n22\n35\n29\n\n\n2\n45\n49\n35\n38\n14\n\n\n3\n41\n26\n22\n37\n17\n\n\n4\n34\n34\n34\n44\n30\n\n\n...\n...\n...\n...\n...\n...\n\n\n19714\n35\n36\n21\n42\n19\n\n\n19715\n30\n32\n25\n36\n39\n\n\n19716\n37\n23\n21\n26\n10\n\n\n19717\n42\n43\n21\n38\n20\n\n\n19718\n49\n36\n24\n35\n23\n\n\n\n\n19719 rows × 5 columns\n\n\n\n\nextroversion_items = ['E1','E2','E3','E4','E5','E6','E7','E8','E9','E10']\nextroversion_data = big5_df[extroversion_items]\n\n\nbig5_df['extroversion_score'] = extroversion_data.mean(axis=1)\n\nAfter, I added age as a variable:\n\nage = big5_df['age']\n\n\nextroversion = big5_df['extroversion_score']\n\n\nclean_data = big5_df[['age','extroversion_score']].dropna()\n\n\ncorr, p_value = pearsonr(clean_data['age'], clean_data['extroversion_score'])\n\nprint(\"Correlation:\", corr)\nprint(\"P-value:\", p_value)\n\nCorrelation: 0.006540007768999916\nP-value: 0.35844717966337664\n\n\n\nbig5_df['age'] = pd.to_numeric(big5_df['age'], errors='coerce')\nbig5_df['age'].describe()\nbig5_df['age'].value_counts(dropna=False).head(10)\n\nage\n18    1523\n17    1370\n19    1259\n20    1231\n21    1216\n16    1148\n22     970\n23     895\n15     743\n24     703\nName: count, dtype: int64\n\n\n\nimport numpy as np\n\nbig5_df['age'] = pd.to_numeric(big5_df['age'], errors='coerce')\n\n\nbig5_df.loc[(big5_df['age'] &lt; 10) | (big5_df['age'] &gt; 100), 'age'] = np.nan\n\nclean = big5_df[['age', 'extroversion_score']].dropna()\n\n\nimport matplotlib.pyplot as plt\n\nplt.scatter(clean['age'], clean['extroversion_score'], alpha=0.4)\nplt.xlabel(\"Age (years)\")\nplt.ylabel(\"Extroversion Score\")\nplt.title(\"Age vs Extroversion\")\nplt.show()\n\n\n\n\n\n\n\n\nThis scatterplot shows the correlation of levels of Age and Extraversion on a 5 point scale. This graph shows a slight negative relationship between age and extraversion, suggesting that extraversion tends to decline modestly across the lifespan. However, it’s hard to tell from this initial graph since the data is heavily concentrated towards youth.\nI then decided to segment the age data by decade to get a clearer depiction on the average extraversion levels from smaller groups.\n\nclean = big5_df[['age','extroversion_score']].dropna()\n\n\nclean['age_decade'] = (clean['age'] // 10) * 10\n\n\ndecade_extro = clean.groupby('age_decade')['extroversion_score'].mean()\n\n\nprint(decade_extro)\n\nage_decade\n10.0     3.074408\n20.0     3.093743\n30.0     3.064531\n40.0     3.055792\n50.0     3.028171\n60.0     3.061565\n70.0     2.997917\n80.0     3.100000\n90.0     3.766667\n100.0    5.000000\nName: extroversion_score, dtype: float64\n\n\n\ndecade_extro.index = decade_extro.index.astype(str) + \"s\"\n\ndecade_extro.plot(kind='bar')\n\nplt.xlabel(\"Age Decade\")\nplt.ylabel(\"Average Extroversion Score\")\nplt.title(\"Average Extroversion by Age Decade\")\nplt.show()\n\n\n\n\n\n\n\n\nThis graph actually counters my initial interpretations from the data, since the bar graph illustrates how extraversion spikes for 90 and 100 year olds whereas extroversion levels stay stagnant and neutral through youth and middle-aged folks. Except this graph doesn’t account for the number of participants surveyed from each decade. In the above scatterplot, most of the datapoints were concentrated towards younger generations, signifying that more youth were surveyed than elderly people.\nBecause of this, I decided to create a final graph illustrating the sample size of each decade.\n\nclean['age_decade'].value_counts().sort_index()\n\nage_decade\n10.0     6756\n20.0     7528\n30.0     2560\n40.0     1554\n50.0      891\n60.0      294\n70.0       48\n80.0        1\n90.0        3\n100.0       1\nName: count, dtype: int64\n\n\n\ndecade_extro = clean.groupby('age_decade')['extroversion_score'].mean()\ndecade_counts = clean.groupby('age_decade').size()\n\n\nimport matplotlib.pyplot as plt\n\nfig, ax1 = plt.subplots()\nax1.bar(decade_counts.index, decade_counts, alpha=0.4)\nax1.set_xlabel(\"Age Decade\")\nax1.set_ylabel(\"Number of Participants\")\nax2 = ax1.twinx()\n\nax2.plot(decade_extro.index, decade_extro, marker='o')\nax2.set_ylabel(\"Average Extroversion Score\")\n\nplt.title(\"Extroversion Trends vs Sample Size by Age Decade\")\nplt.show()\n\n\n\n\n\n\n\n\nThis final graph properly accounts for the correlation between extraversion and age, accounting for sample size in each of these ages that were segmented by decade. This graph suggests that extraversion remains relatively stable across early and middle adulthood, with only minor fluctuations between individuals in their teens through their sixties. Average extraversion scores appear slightly higher during adolescence and early adulthood, followed by a modest decline into middle age. This pattern aligns with established personality research, suggesting that social energy and novelty-seeking behaviors often decrease gradually as individuals age and shift toward smaller, more stable social networks and life roles.\nHowever, the visualization also highlights an important limitation in the dataset: a significant imbalance in participant distribution across age groups. The majority of respondents were concentrated in the teenage and young adult decades, with over several thousand participants represented in these groups. In contrast, participation declines sharply in older decades, with extremely small sample sizes appearing beyond age seventy. This uneven distribution substantially affects the reliability of the observed trends in later adulthood.\nNotably, the graph shows a sharp increase in average extroversion scores among participants in their nineties and beyond. While this appears visually striking, it is likely the result of extremely limited data rather than a meaningful psychological trend. Thus, this information must be interpreted with caution or discounted alto"
  },
  {
    "objectID": "posts/003_DOW2_big5questions/DOW2_big5.html#specifically-the-intersection-between-extraversion-and-age",
    "href": "posts/003_DOW2_big5questions/DOW2_big5.html#specifically-the-intersection-between-extraversion-and-age",
    "title": "Dataset of the Week #2: Exploring the Big 5 Personality Test",
    "section": "",
    "text": "The Big 5 Personality Test is a part of the Open-Source Psychometrics Project, designed to measure individuals across five core dimensions: Extraversion, Emotional Stability, Agreeableness, Conscientiousness, and Intellect/Imagination. It provides a snapshot into these personality traits that impact our behaviors, thoughts, and emotions.\nIn this case, I will be examining the correlation between Extraversion and Age. In the context of this test, Extroversion is said to be, “an individual’s tendency to seek out social interaction and stimulation, as well as their level of enthusiasm and assertiveness in social situations. Individuals who score high on this dimension tend to be outgoing, sociable, and talkative. They enjoy being around others and seek out social situations. They are often described as having a high level of energy, enthusiasm, and assertiveness. They may also be more likely to engage in risk-taking behaviors, such as partying, drinking, or other forms of excitement-seeking. In contrast, individuals who score low on extraversion are more introverted and reserved. They may prefer to spend time alone or in small groups, and may feel uncomfortable in large social gatherings. They may also be less assertive and more cautious in their interactions with others.”\nIt is proven that extraversion is correlated to multiple outcomes, ranging from job performance and social support to overall well-being. We will further explore these conclusions through analyzing the data from this test.\n\nimport pandas as pd\nimport re\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import pearsonr\n\n\nbig5_df = pd.read_csv('data/openpsych_data.csv', sep='\\t')\n\n\nbig5_df\n\n\n\n\n\n\n\n\nrace\nage\nengnat\ngender\nhand\nsource\ncountry\nE1\nE2\nE3\n...\nO1\nO2\nO3\nO4\nO5\nO6\nO7\nO8\nO9\nO10\n\n\n\n\n0\n3\n53\n1\n1\n1\n1\nUS\n4\n2\n5\n...\n4\n1\n3\n1\n5\n1\n4\n2\n5\n5\n\n\n1\n13\n46\n1\n2\n1\n1\nUS\n2\n2\n3\n...\n3\n3\n3\n3\n2\n3\n3\n1\n3\n2\n\n\n2\n1\n14\n2\n2\n1\n1\nPK\n5\n1\n1\n...\n4\n5\n5\n1\n5\n1\n5\n5\n5\n5\n\n\n3\n3\n19\n2\n2\n1\n1\nRO\n2\n5\n2\n...\n4\n3\n5\n2\n4\n2\n5\n2\n5\n5\n\n\n4\n11\n25\n2\n2\n1\n2\nUS\n3\n1\n3\n...\n3\n1\n1\n1\n3\n1\n3\n1\n5\n3\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n19714\n11\n15\n1\n2\n1\n2\nSG\n1\n4\n3\n...\n1\n3\n5\n3\n4\n1\n4\n2\n5\n3\n\n\n19715\n3\n37\n1\n2\n1\n2\nUS\n2\n3\n2\n...\n1\n2\n3\n2\n3\n3\n4\n2\n3\n3\n\n\n19716\n5\n16\n2\n1\n1\n2\nUS\n2\n5\n4\n...\n5\n3\n1\n3\n4\n1\n1\n5\n5\n5\n\n\n19717\n12\n16\n1\n1\n1\n5\nNG\n1\n4\n2\n...\n3\n2\n5\n3\n4\n1\n5\n3\n5\n5\n\n\n19718\n3\n35\n1\n1\n1\n1\nUS\n2\n3\n1\n...\n5\n1\n5\n1\n4\n1\n5\n5\n5\n5\n\n\n\n\n19719 rows × 57 columns\n\n\n\nParticipants of this survey were tasked to answer 50 of the questions below, 10 questions associated with each of the above traits.\n\nquestions = '''\nE1  I am the life of the party.\nE2  I don't talk a lot.\nE3  I feel comfortable around people.\nE4  I keep in the background.\nE5  I start conversations.\nE6  I have little to say.\nE7  I talk to a lot of different people at parties.\nE8  I don't like to draw attention to myself.\nE9  I don't mind being the center of attention.\nE10 I am quiet around strangers.\nN1  I get stressed out easily.\nN2  I am relaxed most of the time.\nN3  I worry about things.\nN4  I seldom feel blue.\nN5  I am easily disturbed.\nN6  I get upset easily.\nN7  I change my mood a lot.\nN8  I have frequent mood swings.\nN9  I get irritated easily.\nN10 I often feel blue.\nA1  I feel little concern for others.\nA2  I am interested in people.\nA3  I insult people.\nA4  I sympathize with others' feelings.\nA5  I am not interested in other people's problems.\nA6  I have a soft heart.\nA7  I am not really interested in others.\nA8  I take time out for others.\nA9  I feel others' emotions.\nA10 I make people feel at ease.\nC1  I am always prepared.\nC2  I leave my belongings around.\nC3  I pay attention to details.\nC4  I make a mess of things.\nC5  I get chores done right away.\nC6  I often forget to put things back in their proper place.\nC7  I like order.\nC8  I shirk my duties.\nC9  I follow a schedule.\nC10 I am exacting in my work.\nO1  I have a rich vocabulary.\nO2  I have difficulty understanding abstract ideas.\nO3  I have a vivid imagination.\nO4  I am not interested in abstract ideas.\nO5  I have excellent ideas.\nO6  I do not have a good imagination.\nO7  I am quick to understand things.\nO8  I use difficult words.\nO9  I spend time reflecting on things.\nO10 I am full of ideas.\n'''\n\n\nbig5_questions_df = pd.DataFrame([item.split('\\t') for item in questions.splitlines() if item&gt;''])\n\n\nbig5_questions_df\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\nE1\nI am the life of the party.\n\n\n1\nE2\nI don't talk a lot.\n\n\n2\nE3\nI feel comfortable around people.\n\n\n3\nE4\nI keep in the background.\n\n\n4\nE5\nI start conversations.\n\n\n5\nE6\nI have little to say.\n\n\n6\nE7\nI talk to a lot of different people at parties.\n\n\n7\nE8\nI don't like to draw attention to myself.\n\n\n8\nE9\nI don't mind being the center of attention.\n\n\n9\nE10\nI am quiet around strangers.\n\n\n10\nN1\nI get stressed out easily.\n\n\n11\nN2\nI am relaxed most of the time.\n\n\n12\nN3\nI worry about things.\n\n\n13\nN4\nI seldom feel blue.\n\n\n14\nN5\nI am easily disturbed.\n\n\n15\nN6\nI get upset easily.\n\n\n16\nN7\nI change my mood a lot.\n\n\n17\nN8\nI have frequent mood swings.\n\n\n18\nN9\nI get irritated easily.\n\n\n19\nN10\nI often feel blue.\n\n\n20\nA1\nI feel little concern for others.\n\n\n21\nA2\nI am interested in people.\n\n\n22\nA3\nI insult people.\n\n\n23\nA4\nI sympathize with others' feelings.\n\n\n24\nA5\nI am not interested in other people's problems.\n\n\n25\nA6\nI have a soft heart.\n\n\n26\nA7\nI am not really interested in others.\n\n\n27\nA8\nI take time out for others.\n\n\n28\nA9\nI feel others' emotions.\n\n\n29\nA10\nI make people feel at ease.\n\n\n30\nC1\nI am always prepared.\n\n\n31\nC2\nI leave my belongings around.\n\n\n32\nC3\nI pay attention to details.\n\n\n33\nC4\nI make a mess of things.\n\n\n34\nC5\nI get chores done right away.\n\n\n35\nC6\nI often forget to put things back in their pro...\n\n\n36\nC7\nI like order.\n\n\n37\nC8\nI shirk my duties.\n\n\n38\nC9\nI follow a schedule.\n\n\n39\nC10\nI am exacting in my work.\n\n\n40\nO1\nI have a rich vocabulary.\n\n\n41\nO2\nI have difficulty understanding abstract ideas.\n\n\n42\nO3\nI have a vivid imagination.\n\n\n43\nO4\nI am not interested in abstract ideas.\n\n\n44\nO5\nI have excellent ideas.\n\n\n45\nO6\nI do not have a good imagination.\n\n\n46\nO7\nI am quick to understand things.\n\n\n47\nO8\nI use difficult words.\n\n\n48\nO9\nI spend time reflecting on things.\n\n\n49\nO10\nI am full of ideas.\n\n\n\n\n\n\n\nFirst, I categorized all of the questions into which trait they fulfill:\n\nfactor_map = { 1: 'E', \n               2: 'A',\n               3: 'C',\n               4: 'N',\n               5: 'O' }\n\n\nipip_df = pd.read_html('big5_questions.html', header=0)[0]\nipip_df = ipip_df.rename(columns={'Unnamed: 1': 'text', 'Unnamed: 7': 'factor_and_direction'})[['text','factor_and_direction']]\nipip_df[['factor','direction']]=ipip_df['factor_and_direction'].str.extract(r'([1-5])(.)')\nipip_df['category']=ipip_df['factor'].astype(int).map(factor_map)\n\n\nipip_df = ipip_df.assign(number=np.repeat(np.arange(1,11),5))\nipip_df = ipip_df.assign(qcode=ipip_df['category'].str.cat(ipip_df['number'].astype(str)))  \n\n\nneg_items = ipip_df.query('direction==\"-\"')['qcode']\nneg_items\n\n1      A1\n3      N1\n5      E2\n7      C2\n9      O2\n11     A3\n13     N3\n15     E4\n17     C4\n19     O4\n21     A5\n23     N5\n25     E6\n27     C6\n28     N6\n29     O6\n31     A7\n33     N7\n35     E8\n37     C8\n38     N8\n43     N9\n45    E10\n48    N10\nName: qcode, dtype: str\n\n\n\nbig5_scored_df = big5_df.copy()\n\n\nbig5_scored_df[neg_items] = 6-big5_df[neg_items]\n\nThen, I sorted the data to just record the Extraversion responses:\n\nE_cols = [f'E{n+1}' for n in range(10)]\nE_cols\n\n['E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8', 'E9', 'E10']\n\n\n\ncat_cols\n\n{'O': ['O1', 'O2', 'O3', 'O4', 'O5', 'O6', 'O7', 'O8', 'O9', 'O10'],\n 'C': ['C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10'],\n 'E': ['E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8', 'E9', 'E10'],\n 'A': ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10'],\n 'N': ['N1', 'N2', 'N3', 'N4', 'N5', 'N6', 'N7', 'N8', 'N9', 'N10']}\n\n\n\nfor cat, cols in cat_cols.items():\n    big5_scored_df[cat]=big5_scored_df[cols].sum(axis=1)\n\n\nbig5_scored_df[['O','C','E','A','N']]\n\n\n\n\n\n\n\n\nO\nC\nE\nA\nN\n\n\n\n\n0\n43\n47\n44\n46\n49\n\n\n1\n26\n42\n22\n35\n29\n\n\n2\n45\n49\n35\n38\n14\n\n\n3\n41\n26\n22\n37\n17\n\n\n4\n34\n34\n34\n44\n30\n\n\n...\n...\n...\n...\n...\n...\n\n\n19714\n35\n36\n21\n42\n19\n\n\n19715\n30\n32\n25\n36\n39\n\n\n19716\n37\n23\n21\n26\n10\n\n\n19717\n42\n43\n21\n38\n20\n\n\n19718\n49\n36\n24\n35\n23\n\n\n\n\n19719 rows × 5 columns\n\n\n\n\nextroversion_items = ['E1','E2','E3','E4','E5','E6','E7','E8','E9','E10']\nextroversion_data = big5_df[extroversion_items]\n\n\nbig5_df['extroversion_score'] = extroversion_data.mean(axis=1)\n\nAfter, I added age as a variable:\n\nage = big5_df['age']\n\n\nextroversion = big5_df['extroversion_score']\n\n\nclean_data = big5_df[['age','extroversion_score']].dropna()\n\n\ncorr, p_value = pearsonr(clean_data['age'], clean_data['extroversion_score'])\n\nprint(\"Correlation:\", corr)\nprint(\"P-value:\", p_value)\n\nCorrelation: 0.006540007768999916\nP-value: 0.35844717966337664\n\n\n\nbig5_df['age'] = pd.to_numeric(big5_df['age'], errors='coerce')\nbig5_df['age'].describe()\nbig5_df['age'].value_counts(dropna=False).head(10)\n\nage\n18    1523\n17    1370\n19    1259\n20    1231\n21    1216\n16    1148\n22     970\n23     895\n15     743\n24     703\nName: count, dtype: int64\n\n\n\nimport numpy as np\n\nbig5_df['age'] = pd.to_numeric(big5_df['age'], errors='coerce')\n\n\nbig5_df.loc[(big5_df['age'] &lt; 10) | (big5_df['age'] &gt; 100), 'age'] = np.nan\n\nclean = big5_df[['age', 'extroversion_score']].dropna()\n\n\nimport matplotlib.pyplot as plt\n\nplt.scatter(clean['age'], clean['extroversion_score'], alpha=0.4)\nplt.xlabel(\"Age (years)\")\nplt.ylabel(\"Extroversion Score\")\nplt.title(\"Age vs Extroversion\")\nplt.show()\n\n\n\n\n\n\n\n\nThis scatterplot shows the correlation of levels of Age and Extraversion on a 5 point scale. This graph shows a slight negative relationship between age and extraversion, suggesting that extraversion tends to decline modestly across the lifespan. However, it’s hard to tell from this initial graph since the data is heavily concentrated towards youth.\nI then decided to segment the age data by decade to get a clearer depiction on the average extraversion levels from smaller groups.\n\nclean = big5_df[['age','extroversion_score']].dropna()\n\n\nclean['age_decade'] = (clean['age'] // 10) * 10\n\n\ndecade_extro = clean.groupby('age_decade')['extroversion_score'].mean()\n\n\nprint(decade_extro)\n\nage_decade\n10.0     3.074408\n20.0     3.093743\n30.0     3.064531\n40.0     3.055792\n50.0     3.028171\n60.0     3.061565\n70.0     2.997917\n80.0     3.100000\n90.0     3.766667\n100.0    5.000000\nName: extroversion_score, dtype: float64\n\n\n\ndecade_extro.index = decade_extro.index.astype(str) + \"s\"\n\ndecade_extro.plot(kind='bar')\n\nplt.xlabel(\"Age Decade\")\nplt.ylabel(\"Average Extroversion Score\")\nplt.title(\"Average Extroversion by Age Decade\")\nplt.show()\n\n\n\n\n\n\n\n\nThis graph actually counters my initial interpretations from the data, since the bar graph illustrates how extraversion spikes for 90 and 100 year olds whereas extroversion levels stay stagnant and neutral through youth and middle-aged folks. Except this graph doesn’t account for the number of participants surveyed from each decade. In the above scatterplot, most of the datapoints were concentrated towards younger generations, signifying that more youth were surveyed than elderly people.\nBecause of this, I decided to create a final graph illustrating the sample size of each decade.\n\nclean['age_decade'].value_counts().sort_index()\n\nage_decade\n10.0     6756\n20.0     7528\n30.0     2560\n40.0     1554\n50.0      891\n60.0      294\n70.0       48\n80.0        1\n90.0        3\n100.0       1\nName: count, dtype: int64\n\n\n\ndecade_extro = clean.groupby('age_decade')['extroversion_score'].mean()\ndecade_counts = clean.groupby('age_decade').size()\n\n\nimport matplotlib.pyplot as plt\n\nfig, ax1 = plt.subplots()\nax1.bar(decade_counts.index, decade_counts, alpha=0.4)\nax1.set_xlabel(\"Age Decade\")\nax1.set_ylabel(\"Number of Participants\")\nax2 = ax1.twinx()\n\nax2.plot(decade_extro.index, decade_extro, marker='o')\nax2.set_ylabel(\"Average Extroversion Score\")\n\nplt.title(\"Extroversion Trends vs Sample Size by Age Decade\")\nplt.show()\n\n\n\n\n\n\n\n\nThis final graph properly accounts for the correlation between extraversion and age, accounting for sample size in each of these ages that were segmented by decade. This graph suggests that extraversion remains relatively stable across early and middle adulthood, with only minor fluctuations between individuals in their teens through their sixties. Average extraversion scores appear slightly higher during adolescence and early adulthood, followed by a modest decline into middle age. This pattern aligns with established personality research, suggesting that social energy and novelty-seeking behaviors often decrease gradually as individuals age and shift toward smaller, more stable social networks and life roles.\nHowever, the visualization also highlights an important limitation in the dataset: a significant imbalance in participant distribution across age groups. The majority of respondents were concentrated in the teenage and young adult decades, with over several thousand participants represented in these groups. In contrast, participation declines sharply in older decades, with extremely small sample sizes appearing beyond age seventy. This uneven distribution substantially affects the reliability of the observed trends in later adulthood.\nNotably, the graph shows a sharp increase in average extroversion scores among participants in their nineties and beyond. While this appears visually striking, it is likely the result of extremely limited data rather than a meaningful psychological trend. Thus, this information must be interpreted with caution or discounted alto"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Playing with LLM Prompts",
    "section": "",
    "text": "Exploring the BIG5 dataset from the Open-Source Psychometrics Project\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDataset of the Week #2: Exploring the Big 5 Personality Test\n\n\n\nDOW\n\nOpen-source Psychometrics Project\n\nPython\n\n\n\n\n\n\n\n\n\nFeb 12, 2026\n\n\nMaggie Goldman\n\n\n\n\n\n\n\n\n\n\n\n\nDataset of the Week #1: The Weather in Philadelphia\n\n\n\nDOW\n\nWeather\n\nPython\n\n\n\n\n\n\n\n\n\nFeb 6, 2026\n\n\nMaggie\n\n\n\n\n\n\n\n\n\n\n\n\nMaggie’s Favorite Recipes\n\n\n\nFood\n\nCooking\n\nCollege\n\n\n\nHow Maggie utilizes her kitchen as a sophomore living in Harnwell.\n\n\n\n\n\nJan 28, 2026\n\n\nMaggie Goldman\n\n\n\n\n\n\n\n\n\n\n\n\nA test post\n\n\n\nvisualization\n\ndata stories\n\n\n\nAn example post from a Jupyter notebook\n\n\n\n\n\nJan 12, 2026\n\n\nAn LLM User\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/003_DOW2_big5questions/DOW2_notebook.html",
    "href": "posts/003_DOW2_big5questions/DOW2_notebook.html",
    "title": "Exploring the BIG5 dataset from the Open-Source Psychometrics Project",
    "section": "",
    "text": "import pandas as pd\nimport re\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom IPython.display import display\nbig5_df = pd.read_csv('data/openpsych_data.csv', sep='\\t')\nbig5_df.columns\n\nIndex(['race', 'age', 'engnat', 'gender', 'hand', 'source', 'country', 'E1',\n       'E2', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8', 'E9', 'E10', 'N1', 'N2', 'N3',\n       'N4', 'N5', 'N6', 'N7', 'N8', 'N9', 'N10', 'A1', 'A2', 'A3', 'A4', 'A5',\n       'A6', 'A7', 'A8', 'A9', 'A10', 'C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7',\n       'C8', 'C9', 'C10', 'O1', 'O2', 'O3', 'O4', 'O5', 'O6', 'O7', 'O8', 'O9',\n       'O10'],\n      dtype='str')\nbig5_df\n\n\n\n\n\n\n\n\nrace\nage\nengnat\ngender\nhand\nsource\ncountry\nE1\nE2\nE3\n...\nO1\nO2\nO3\nO4\nO5\nO6\nO7\nO8\nO9\nO10\n\n\n\n\n0\n3\n53\n1\n1\n1\n1\nUS\n4\n2\n5\n...\n4\n1\n3\n1\n5\n1\n4\n2\n5\n5\n\n\n1\n13\n46\n1\n2\n1\n1\nUS\n2\n2\n3\n...\n3\n3\n3\n3\n2\n3\n3\n1\n3\n2\n\n\n2\n1\n14\n2\n2\n1\n1\nPK\n5\n1\n1\n...\n4\n5\n5\n1\n5\n1\n5\n5\n5\n5\n\n\n3\n3\n19\n2\n2\n1\n1\nRO\n2\n5\n2\n...\n4\n3\n5\n2\n4\n2\n5\n2\n5\n5\n\n\n4\n11\n25\n2\n2\n1\n2\nUS\n3\n1\n3\n...\n3\n1\n1\n1\n3\n1\n3\n1\n5\n3\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n19714\n11\n15\n1\n2\n1\n2\nSG\n1\n4\n3\n...\n1\n3\n5\n3\n4\n1\n4\n2\n5\n3\n\n\n19715\n3\n37\n1\n2\n1\n2\nUS\n2\n3\n2\n...\n1\n2\n3\n2\n3\n3\n4\n2\n3\n3\n\n\n19716\n5\n16\n2\n1\n1\n2\nUS\n2\n5\n4\n...\n5\n3\n1\n3\n4\n1\n1\n5\n5\n5\n\n\n19717\n12\n16\n1\n1\n1\n5\nNG\n1\n4\n2\n...\n3\n2\n5\n3\n4\n1\n5\n3\n5\n5\n\n\n19718\n3\n35\n1\n1\n1\n1\nUS\n2\n3\n1\n...\n5\n1\n5\n1\n4\n1\n5\n5\n5\n5\n\n\n\n\n19719 rows × 57 columns"
  },
  {
    "objectID": "posts/003_DOW2_big5questions/DOW2_notebook.html#scoring",
    "href": "posts/003_DOW2_big5questions/DOW2_notebook.html#scoring",
    "title": "Exploring the BIG5 dataset from the Open-Source Psychometrics Project",
    "section": "Scoring",
    "text": "Scoring\n\nhttps://ipip.ori.org/newScoringInstructions.htm\n\n\nHere is how to score IPIP scales:\nFor + keyed items, the response “Very Inaccurate” is assigned a value of 1, “Moderately Inaccurate” a value of 2, “Neither Inaccurate nor Accurate” a 3, “Moderately Accurate” a 4, and “Very Accurate” a value of 5.\nFor - keyed items, the response “Very Inaccurate” is assigned a value of 5, “Moderately Inaccurate” a value of 4, “Neither Inaccurate nor Accurate” a 3, “Moderately Accurate” a 2, and “Very Accurate” a value of 1.\nOnce numbers are assigned for all of the items in the scale, just sum all the values to obtain a total scale score.\n\n\n\nKeyed items\nhttps://ipip.ori.org/new_ipip-50-item-scale.htm\n\nquestions = '''\nE1  I am the life of the party.\nE2  I don't talk a lot.\nE3  I feel comfortable around people.\nE4  I keep in the background.\nE5  I start conversations.\nE6  I have little to say.\nE7  I talk to a lot of different people at parties.\nE8  I don't like to draw attention to myself.\nE9  I don't mind being the center of attention.\nE10 I am quiet around strangers.\nN1  I get stressed out easily.\nN2  I am relaxed most of the time.\nN3  I worry about things.\nN4  I seldom feel blue.\nN5  I am easily disturbed.\nN6  I get upset easily.\nN7  I change my mood a lot.\nN8  I have frequent mood swings.\nN9  I get irritated easily.\nN10 I often feel blue.\nA1  I feel little concern for others.\nA2  I am interested in people.\nA3  I insult people.\nA4  I sympathize with others' feelings.\nA5  I am not interested in other people's problems.\nA6  I have a soft heart.\nA7  I am not really interested in others.\nA8  I take time out for others.\nA9  I feel others' emotions.\nA10 I make people feel at ease.\nC1  I am always prepared.\nC2  I leave my belongings around.\nC3  I pay attention to details.\nC4  I make a mess of things.\nC5  I get chores done right away.\nC6  I often forget to put things back in their proper place.\nC7  I like order.\nC8  I shirk my duties.\nC9  I follow a schedule.\nC10 I am exacting in my work.\nO1  I have a rich vocabulary.\nO2  I have difficulty understanding abstract ideas.\nO3  I have a vivid imagination.\nO4  I am not interested in abstract ideas.\nO5  I have excellent ideas.\nO6  I do not have a good imagination.\nO7  I am quick to understand things.\nO8  I use difficult words.\nO9  I spend time reflecting on things.\nO10 I am full of ideas.\n'''\n\n\nbig5_questions_df = pd.DataFrame([item.split('\\t') for item in questions.splitlines() if item&gt;''])\n\n\nbig5_questions_df\n\n\n\n\n\n\n\n\n0\n1\n\n\n\n\n0\nE1\nI am the life of the party.\n\n\n1\nE2\nI don't talk a lot.\n\n\n2\nE3\nI feel comfortable around people.\n\n\n3\nE4\nI keep in the background.\n\n\n4\nE5\nI start conversations.\n\n\n5\nE6\nI have little to say.\n\n\n6\nE7\nI talk to a lot of different people at parties.\n\n\n7\nE8\nI don't like to draw attention to myself.\n\n\n8\nE9\nI don't mind being the center of attention.\n\n\n9\nE10\nI am quiet around strangers.\n\n\n10\nN1\nI get stressed out easily.\n\n\n11\nN2\nI am relaxed most of the time.\n\n\n12\nN3\nI worry about things.\n\n\n13\nN4\nI seldom feel blue.\n\n\n14\nN5\nI am easily disturbed.\n\n\n15\nN6\nI get upset easily.\n\n\n16\nN7\nI change my mood a lot.\n\n\n17\nN8\nI have frequent mood swings.\n\n\n18\nN9\nI get irritated easily.\n\n\n19\nN10\nI often feel blue.\n\n\n20\nA1\nI feel little concern for others.\n\n\n21\nA2\nI am interested in people.\n\n\n22\nA3\nI insult people.\n\n\n23\nA4\nI sympathize with others' feelings.\n\n\n24\nA5\nI am not interested in other people's problems.\n\n\n25\nA6\nI have a soft heart.\n\n\n26\nA7\nI am not really interested in others.\n\n\n27\nA8\nI take time out for others.\n\n\n28\nA9\nI feel others' emotions.\n\n\n29\nA10\nI make people feel at ease.\n\n\n30\nC1\nI am always prepared.\n\n\n31\nC2\nI leave my belongings around.\n\n\n32\nC3\nI pay attention to details.\n\n\n33\nC4\nI make a mess of things.\n\n\n34\nC5\nI get chores done right away.\n\n\n35\nC6\nI often forget to put things back in their pro...\n\n\n36\nC7\nI like order.\n\n\n37\nC8\nI shirk my duties.\n\n\n38\nC9\nI follow a schedule.\n\n\n39\nC10\nI am exacting in my work.\n\n\n40\nO1\nI have a rich vocabulary.\n\n\n41\nO2\nI have difficulty understanding abstract ideas.\n\n\n42\nO3\nI have a vivid imagination.\n\n\n43\nO4\nI am not interested in abstract ideas.\n\n\n44\nO5\nI have excellent ideas.\n\n\n45\nO6\nI do not have a good imagination.\n\n\n46\nO7\nI am quick to understand things.\n\n\n47\nO8\nI use difficult words.\n\n\n48\nO9\nI spend time reflecting on things.\n\n\n49\nO10\nI am full of ideas.\n\n\n\n\n\n\n\n\n\n\nQuestions and direction key\nhttps://ipip.ori.org/new_ipip-50-item-scale.htm\n\nfactor_map = { 1: 'E', \n               2: 'A',\n               3: 'C',\n               4: 'N',\n               5: 'O' }\n\n\nipip_df = pd.read_html('big5_questions.html', header=0)[0]\nipip_df = ipip_df.rename(columns={'Unnamed: 1': 'text', 'Unnamed: 7': 'factor_and_direction'})[['text','factor_and_direction']]\nipip_df[['factor','direction']]=ipip_df['factor_and_direction'].str.extract(r'([1-5])(.)')\nipip_df['category']=ipip_df['factor'].astype(int).map(factor_map)\n\n\nipip_df = ipip_df.assign(number=np.repeat(np.arange(1,11),5))\nipip_df = ipip_df.assign(qcode=ipip_df['category'].str.cat(ipip_df['number'].astype(str)))   \n\n\nneg_items = ipip_df.query('direction==\"-\"')['qcode']\nneg_items\n\n1      A1\n3      N1\n5      E2\n7      C2\n9      O2\n11     A3\n13     N3\n15     E4\n17     C4\n19     O4\n21     A5\n23     N5\n25     E6\n27     C6\n28     N6\n29     O6\n31     A7\n33     N7\n35     E8\n37     C8\n38     N8\n43     N9\n45    E10\n48    N10\nName: qcode, dtype: str\n\n\n\nMake a copy of the original dataframe to keep available in case of mistakes\n\n\nbig5_scored_df = big5_df.copy()\n\n\nReverse code the negatively keyed items\n\n\nbig5_scored_df[neg_items] = 6-big5_df[neg_items]\n\n\nE_cols = [f'E{n+1}' for n in range(10)]\nE_cols\n\n['E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8', 'E9', 'E10']\n\n\n\ncat_cols = {\n    cat : [f'{cat}{n+1}' for n in range(10)] \n    for cat in ('O','C','E','A','N') \n}\n\n\ncat_cols\n\n{'O': ['O1', 'O2', 'O3', 'O4', 'O5', 'O6', 'O7', 'O8', 'O9', 'O10'],\n 'C': ['C1', 'C2', 'C3', 'C4', 'C5', 'C6', 'C7', 'C8', 'C9', 'C10'],\n 'E': ['E1', 'E2', 'E3', 'E4', 'E5', 'E6', 'E7', 'E8', 'E9', 'E10'],\n 'A': ['A1', 'A2', 'A3', 'A4', 'A5', 'A6', 'A7', 'A8', 'A9', 'A10'],\n 'N': ['N1', 'N2', 'N3', 'N4', 'N5', 'N6', 'N7', 'N8', 'N9', 'N10']}\n\n\n\nfor cat, cols in cat_cols.items():\n    big5_scored_df[cat]=big5_scored_df[cols].sum(axis=1)\n\n\nbig5_scored_df[['O','C','E','A','N']]\n\n\n\n\n\n\n\n\nO\nC\nE\nA\nN\n\n\n\n\n0\n43\n47\n44\n46\n49\n\n\n1\n26\n42\n22\n35\n29\n\n\n2\n45\n49\n35\n38\n14\n\n\n3\n41\n26\n22\n37\n17\n\n\n4\n34\n34\n34\n44\n30\n\n\n...\n...\n...\n...\n...\n...\n\n\n19714\n35\n36\n21\n42\n19\n\n\n19715\n30\n32\n25\n36\n39\n\n\n19716\n37\n23\n21\n26\n10\n\n\n19717\n42\n43\n21\n38\n20\n\n\n19718\n49\n36\n24\n35\n23\n\n\n\n\n19719 rows × 5 columns\n\n\n\n\ngender_dist_df = (\n    big5_scored_df.\n        groupby('gender_cat')[['O','C','E','A','N']].\n        agg('mean').\n        round(1)\n)\n\ngender_dist_df\n\n\n\n\n\n\n\n\nO\nC\nE\nA\nN\n\n\ngender_cat\n\n\n\n\n\n\n\n\n\nfemale\n38.6\n33.6\n30.5\n39.7\n27.9\n\n\nmale\n39.9\n33.2\n29.5\n36.5\n30.9\n\n\nother\n41.5\n31.4\n24.3\n35.3\n25.9\n\n\n\n\n\n\n\n\ngender_dist_df.transpose().plot(kind='bar')\n\n\n\n\n\n\n\n\n\ngender_dist_df.transpose()\n\n\n\n\n\n\n\ngender_cat\nfemale\nmale\nother\n\n\n\n\nO\n38.6\n39.9\n41.5\n\n\nC\n33.6\n33.2\n31.4\n\n\nE\n30.5\n29.5\n24.3\n\n\nA\n39.7\n36.5\n35.3\n\n\nN\n27.9\n30.9\n25.9\n\n\n\n\n\n\n\n\nindia_df = \n\n\n  Cell In[71], line 1\n    india_df =\n               ^\nSyntaxError: invalid syntax\n\n\n\n\n\nindia_df.shape\n\n\n(\n    big5_scored_df.query('country in [\"US\",\"GB\",\"IN\",\"AU\",\"CA\"]').\n        groupby('country')[['O','C','E','A','N']].\n        agg('mean').\n        round(1).\n        transpose().\n        plot(kind='barh')\n)\n\n\n big5_scored_df.query('country==\"IN\"').shape\n\n\nbig5_scored_df.query('country in [\"US\",\"GB\"]')"
  },
  {
    "objectID": "posts/000_test_post/index.html",
    "href": "posts/000_test_post/index.html",
    "title": "A test post",
    "section": "",
    "text": "import matplotlib.pyplot as plt"
  },
  {
    "objectID": "posts/000_test_post/index.html#stories-from-data",
    "href": "posts/000_test_post/index.html#stories-from-data",
    "title": "A test post",
    "section": "Stories from data",
    "text": "Stories from data\n\nLet’s set up some data\n\n\nx = ['A', 'B', 'C']\ny = [1, 5, 3]\n\n\nNow let’s visualize it\n\nplt.bar(x, y)\nplt.show()"
  },
  {
    "objectID": "posts/001_DOW1_Weather/DOW1.html",
    "href": "posts/001_DOW1_Weather/DOW1.html",
    "title": "Dataset of the Week #1: The Weather in Philadelphia",
    "section": "",
    "text": "Philadelphia Weather on Valentine’s Day from the Past 20 Years\n\nRecords from 2005-2025\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nweather_df = pd.read_csv('data/philadelphia_weather_2005_to_2025.csv')\nweather_df = weather_df.assign(date=pd.to_datetime(weather_df['date']))\n\n\nvalentines_weather = weather_df[ (weather_df['date'].dt.month == 2) & (weather_df['date'].dt.day == 14) ]\nvalentines_weather\n\n\n\n\n\n\n\n\nday\nyear\nhigh\nlow\nrain\nsnow\ndate\nday_num\nmonth_num\ndow\ndow_name\nmonth\n\n\n\n\n44\nFebruary 14\n2005\n52\n35\n0.97\n0.0\n2005-02-14\n14\n2\n0\nMonday\nFebruary\n\n\n409\nFebruary 14\n2006\n41\n25\n0.00\n0.0\n2006-02-14\n14\n2\n1\nTuesday\nFebruary\n\n\n774\nFebruary 14\n2007\n35\n19\n0.59\n1.4\n2007-02-14\n14\n2\n2\nWednesday\nFebruary\n\n\n1139\nFebruary 14\n2008\n40\n27\n0.00\n0.0\n2008-02-14\n14\n2\n3\nThursday\nFebruary\n\n\n1505\nFebruary 14\n2009\n42\n30\n0.00\n0.0\n2009-02-14\n14\n2\n5\nSaturday\nFebruary\n\n\n1870\nFebruary 14\n2010\n37\n23\n0.00\n0.0\n2010-02-14\n14\n2\n6\nSunday\nFebruary\n\n\n2235\nFebruary 14\n2011\n62\n42\n0.00\n0.0\n2011-02-14\n14\n2\n0\nMonday\nFebruary\n\n\n2600\nFebruary 14\n2012\n52\n33\n0.00\n0.0\n2012-02-14\n14\n2\n1\nTuesday\nFebruary\n\n\n2966\nFebruary 14\n2013\n47\n32\n0.00\n0.0\n2013-02-14\n14\n2\n3\nThursday\nFebruary\n\n\n3331\nFebruary 14\n2014\n44\n31\n0.21\n0.7\n2014-02-14\n14\n2\n4\nFriday\nFebruary\n\n\n3696\nFebruary 14\n2015\n32\n18\n0.04\n1.3\n2015-02-14\n14\n2\n5\nSaturday\nFebruary\n\n\n4061\nFebruary 14\n2016\n20\n8\n0.00\n0.0\n2016-02-14\n14\n2\n6\nSunday\nFebruary\n\n\n4427\nFebruary 14\n2017\n47\n28\n0.00\n0.0\n2017-02-14\n14\n2\n1\nTuesday\nFebruary\n\n\n4792\nFebruary 14\n2018\n52\n35\n0.05\n0.0\n2018-02-14\n14\n2\n2\nWednesday\nFebruary\n\n\n5157\nFebruary 14\n2019\n49\n32\n0.00\n0.0\n2019-02-14\n14\n2\n3\nThursday\nFebruary\n\n\n5522\nFebruary 14\n2020\n41\n20\n0.00\n0.0\n2020-02-14\n14\n2\n4\nFriday\nFebruary\n\n\n5888\nFebruary 14\n2021\n36\n26\n0.00\n0.0\n2021-02-14\n14\n2\n6\nSunday\nFebruary\n\n\n6253\nFebruary 14\n2022\n30\n19\n0.00\n0.0\n2022-02-14\n14\n2\n0\nMonday\nFebruary\n\n\n6618\nFebruary 14\n2023\n56\n40\n0.00\n0.0\n2023-02-14\n14\n2\n1\nTuesday\nFebruary\n\n\n6983\nFebruary 14\n2024\n40\n31\n0.00\n0.0\n2024-02-14\n14\n2\n2\nWednesday\nFebruary\n\n\n7349\nFebruary 14\n2025\n38\n29\n0.00\n0.0\n2025-02-14\n14\n2\n4\nFriday\nFebruary\n\n\n\n\n\n\n\n\nvalentines_weather['high']\nvalentines_high = valentines_weather['high']\nvalentines_high\n\n44      52\n409     41\n774     35\n1139    40\n1505    42\n1870    37\n2235    62\n2600    52\n2966    47\n3331    44\n3696    32\n4061    20\n4427    47\n4792    52\n5157    49\n5522    41\n5888    36\n6253    30\n6618    56\n6983    40\n7349    38\nName: high, dtype: int64\n\n\n\nvalentines_high['date'] = pd.to_datetime(valentines_high['date'])\nvalentines_high['date']\n\n44     2005-02-14\n409    2006-02-14\n774    2007-02-14\n1139   2008-02-14\n1505   2009-02-14\n1870   2010-02-14\n2235   2011-02-14\n2600   2012-02-14\n2966   2013-02-14\n3331   2014-02-14\n3696   2015-02-14\n4061   2016-02-14\n4427   2017-02-14\n4792   2018-02-14\n5157   2019-02-14\n5522   2020-02-14\n5888   2021-02-14\n6253   2022-02-14\n6618   2023-02-14\n6983   2024-02-14\n7349   2025-02-14\nName: date, dtype: datetime64[us]\n\n\n\nvalentines_high['year'] = valentines_high['date'].dt.year\nvalentines_high['year']\n\n44      2005\n409     2006\n774     2007\n1139    2008\n1505    2009\n1870    2010\n2235    2011\n2600    2012\n2966    2013\n3331    2014\n3696    2015\n4061    2016\n4427    2017\n4792    2018\n5157    2019\n5522    2020\n5888    2021\n6253    2022\n6618    2023\n6983    2024\n7349    2025\nName: date, dtype: int32\n\n\n\nvalentines_weather.query(\"day=='February 14'\")\n\n\n\n\n\n\n\n\nday\nyear\nhigh\nlow\nrain\nsnow\ndate\nday_num\nmonth_num\ndow\ndow_name\nmonth\n\n\n\n\n44\nFebruary 14\n2005\n52\n35\n0.97\n0.0\n2005-02-14\n14\n2\n0\nMonday\nFebruary\n\n\n409\nFebruary 14\n2006\n41\n25\n0.00\n0.0\n2006-02-14\n14\n2\n1\nTuesday\nFebruary\n\n\n774\nFebruary 14\n2007\n35\n19\n0.59\n1.4\n2007-02-14\n14\n2\n2\nWednesday\nFebruary\n\n\n1139\nFebruary 14\n2008\n40\n27\n0.00\n0.0\n2008-02-14\n14\n2\n3\nThursday\nFebruary\n\n\n1505\nFebruary 14\n2009\n42\n30\n0.00\n0.0\n2009-02-14\n14\n2\n5\nSaturday\nFebruary\n\n\n1870\nFebruary 14\n2010\n37\n23\n0.00\n0.0\n2010-02-14\n14\n2\n6\nSunday\nFebruary\n\n\n2235\nFebruary 14\n2011\n62\n42\n0.00\n0.0\n2011-02-14\n14\n2\n0\nMonday\nFebruary\n\n\n2600\nFebruary 14\n2012\n52\n33\n0.00\n0.0\n2012-02-14\n14\n2\n1\nTuesday\nFebruary\n\n\n2966\nFebruary 14\n2013\n47\n32\n0.00\n0.0\n2013-02-14\n14\n2\n3\nThursday\nFebruary\n\n\n3331\nFebruary 14\n2014\n44\n31\n0.21\n0.7\n2014-02-14\n14\n2\n4\nFriday\nFebruary\n\n\n3696\nFebruary 14\n2015\n32\n18\n0.04\n1.3\n2015-02-14\n14\n2\n5\nSaturday\nFebruary\n\n\n4061\nFebruary 14\n2016\n20\n8\n0.00\n0.0\n2016-02-14\n14\n2\n6\nSunday\nFebruary\n\n\n4427\nFebruary 14\n2017\n47\n28\n0.00\n0.0\n2017-02-14\n14\n2\n1\nTuesday\nFebruary\n\n\n4792\nFebruary 14\n2018\n52\n35\n0.05\n0.0\n2018-02-14\n14\n2\n2\nWednesday\nFebruary\n\n\n5157\nFebruary 14\n2019\n49\n32\n0.00\n0.0\n2019-02-14\n14\n2\n3\nThursday\nFebruary\n\n\n5522\nFebruary 14\n2020\n41\n20\n0.00\n0.0\n2020-02-14\n14\n2\n4\nFriday\nFebruary\n\n\n5888\nFebruary 14\n2021\n36\n26\n0.00\n0.0\n2021-02-14\n14\n2\n6\nSunday\nFebruary\n\n\n6253\nFebruary 14\n2022\n30\n19\n0.00\n0.0\n2022-02-14\n14\n2\n0\nMonday\nFebruary\n\n\n6618\nFebruary 14\n2023\n56\n40\n0.00\n0.0\n2023-02-14\n14\n2\n1\nTuesday\nFebruary\n\n\n6983\nFebruary 14\n2024\n40\n31\n0.00\n0.0\n2024-02-14\n14\n2\n2\nWednesday\nFebruary\n\n\n7349\nFebruary 14\n2025\n38\n29\n0.00\n0.0\n2025-02-14\n14\n2\n4\nFriday\nFebruary\n\n\n\n\n\n\n\n\nvalentines_weather.query(\"day=='February 14'\").set_index('year')[['high','low']]\nval_high_low = valentines_weather.query(\"day=='February 14'\").set_index('year')[['high','low']]\nval_high_low\n\n\n\n\n\n\n\n\nhigh\nlow\n\n\nyear\n\n\n\n\n\n\n2005\n52\n35\n\n\n2006\n41\n25\n\n\n2007\n35\n19\n\n\n2008\n40\n27\n\n\n2009\n42\n30\n\n\n2010\n37\n23\n\n\n2011\n62\n42\n\n\n2012\n52\n33\n\n\n2013\n47\n32\n\n\n2014\n44\n31\n\n\n2015\n32\n18\n\n\n2016\n20\n8\n\n\n2017\n47\n28\n\n\n2018\n52\n35\n\n\n2019\n49\n32\n\n\n2020\n41\n20\n\n\n2021\n36\n26\n\n\n2022\n30\n19\n\n\n2023\n56\n40\n\n\n2024\n40\n31\n\n\n2025\n38\n29\n\n\n\n\n\n\n\n\nval_high_low.plot(kind='line', style='-o')\nplt.title('Philadelphia High and Low temperatures on Valentines Day 2005-2025')\nplt.show()\n\n\n\n\n\n\n\n\n\nwarmest_valentines = valentines_weather['high'].max()\nwarmest_valentines\n\nnp.int64(62)\n\n\n\nmax_idx = valentines_weather['high'].idxmax()\nvalentines_weather.loc[max_idx]\n\nday                  February 14\nyear                        2011\nhigh                          62\nlow                           42\nrain                         0.0\nsnow                         0.0\ndate         2011-02-14 00:00:00\nday_num                       14\nmonth_num                      2\ndow                            0\ndow_name                  Monday\nmonth                   February\nName: 2235, dtype: object\n\n\nThe highest temperature over the past 20 years on Valentine’s day was in 2011, reaching 62 degrees.\n\ncoldest_valentines = valentines_weather['low'].min()\ncoldest_valentines\n\nnp.int64(8)\n\n\n\nmin_idx = valentines_weather['low'].idxmin()\nvalentines_weather.loc[min_idx]\n\nday                  February 14\nyear                        2016\nhigh                          20\nlow                            8\nrain                         0.0\nsnow                         0.0\ndate         2016-02-14 00:00:00\nday_num                       14\nmonth_num                      2\ndow                            6\ndow_name                  Sunday\nmonth                   February\nName: 4061, dtype: object\n\n\nThe lowest temperature over the past 20 years on Valentine’s day was in 2016, reaching only 20 degrees.\n\nis_rain = valentines_weather['rain']&gt;0\nis_rain.sum()\n\nnp.int64(5)\n\n\nThere were 5 Valentine’s days in the past 20 years where there was rain.\n\nprob_rain = (valentines_weather['rain']&gt;0).mean()\nprob_rain*100\n\nnp.float64(23.809523809523807)\n\n\n\nprint(f\"The probability of rain on Valentine's Day: {prob_rain:.23%}\")\n\nThe probability of rain on Valentine's Day: 23.80952380952380664780321%\n\n\n\nis_snow = valentines_weather['snow']&gt;0\nis_snow.sum()\n\nnp.int64(3)\n\n\nThere were 3 Valentine’s days in the past 20 years where there was snow.\n\nprob_snow = (valentines_weather['snow']&gt;0).mean()\nprob_snow*100\n\nnp.float64(14.285714285714285)\n\n\n\nprint(f\"The probability of snow on Valentine's Day: {prob_snow:.14%}\")\n\nThe probability of snow on Valentine's Day: 14.28571428571428%\n\n\n\nConclusion\n\nOverall, there’s been a variety of temperatures in Philadelphia on Valentine’s Day, ranging from a high of 62 degrees to a high of 20 degrees. With that, there’s a variety of activities to do rain, snow, or shine with loved ones, friends, and family! * For a higher temperature Valentine’s Day, you can take a nice stroll through Rittenhouse! * For a lower temperature Valentine’s Day, you can bundle up or stay indoors, trying new cozy food spots!"
  }
]